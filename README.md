# Twelve Labs Quickstarts
Twelve Labs Multimodal AI brings human-like video understanding to any application. Twelve Labs Video Understanding Platform, offers an API and SDKs for integrating state-of-the-art (“SOTA”) video understanding foundation models that understands contextual, spatial and temporal information from your videos.

Just add the SDK ([Python](https://github.com/twelvelabs-io/twelvelabs-python), [JS](https://github.com/twelvelabs-io/twelvelabs-js) and with a few lines of code you can add video understanding in your application.

![image](https://github.com/manish-tl/twelvelabs-cookbook/assets/168238935/630ae2b5-c979-4394-b56e-0e55a373a80c)


* Learn about Twelve Labs at https://www.twelvelabs.io/
* Documentation at https://docs.twelvelabs.io/docs/introduction
* Playground at https://playground.twelvelabs.io/

These tutorials show you how to Start with [Search](https://github.com/manish-tl/twelvelabs-cookbook/blob/main/quickstarts/TwelveLabs_Quickstart_Search.ipynb), [Generate](https://github.com/manish-tl/twelvelabs-cookbook/blob/main/quickstarts/TwelveLabs_Quickstart_Generate.ipynb) and [Embed](https://github.com/manish-tl/twelvelabs-cookbook/blob/main/quickstarts/TwelveLabs_Quickstart_Embeddings.ipynb).
