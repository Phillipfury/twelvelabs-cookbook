# Twelve Labs Quickstarts
Twelve Labs Multimodal AI brings human-like video understanding to any application. Twelve Labs Video Understanding Platform, offers an API and SDKs for integrating state-of-the-art (“SOTA”) video understanding foundation models that understands contextual, spatial and temporal information from your videos.

Just add the SDK ([Python](https://github.com/twelvelabs-io/twelvelabs-python), [JS](https://github.com/twelvelabs-io/twelvelabs-js) and with a few lines of code you can add video understanding in your application.

The following diagram illustrates the architecture of the Twelve Labs Video Understanding Platform and how different parts interact:
Learn about Twelve Labs at https://www.twelvelabs.io/
Documentation at https://docs.twelvelabs.io/docs/introduction
Playground at https://playground.twelvelabs.io/

These tutorials show you how to Start with [Search](https://github.com/manish-tl/twelvelabs-cookbook/blob/main/quickstarts/TwelveLabs_Quickstart_Search.ipynb), [Generate](https://github.com/manish-tl/twelvelabs-cookbook/blob/main/quickstarts/TwelveLabs_Quickstart_Generate.ipynb) and [Embed](https://github.com/manish-tl/twelvelabs-cookbook/blob/main/quickstarts/TwelveLabs_Quickstart_Embeddings.ipynb).
